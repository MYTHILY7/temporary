import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
from config import WEBSITE_URLS
from dateparser import parse

KEYWORDS = ['ai', 'artificial intelligence', 'machine learning', 'deep learning']
CUTOFF_DATE = datetime.now() - timedelta(days=7)

def is_recent(date_str):
    try:
        parsed_date = parse(date_str)
        return parsed_date and parsed_date > CUTOFF_DATE
    except:
        return False

def extract_article_links(base_url):
    try:
        res = requests.get(base_url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = set()
        for a in soup.find_all('a', href=True):
            href = a['href']
            if any(x in href.lower() for x in ['article', 'news', 'ai']) and not href.startswith('#'):
                full_url = href if href.startswith('http') else requests.compat.urljoin(base_url, href)
                links.add(full_url)
        return list(links)[:10]
    except Exception as e:
        print(f"‚ùå Error fetching links from {base_url}: {e}")
        return []

def extract_article_data(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')

        title = soup.title.text.strip() if soup.title else "No title"
        content = ' '.join([p.text for p in soup.find_all('p')])

        meta = soup.find('meta', {'property': 'article:published_time'}) or \
               soup.find('meta', {'name': 'pubdate'}) or \
               soup.find('meta', {'name': 'date'}) or \
               soup.find('time')

        date_str = meta.get('content') if meta and meta.has_attr('content') else meta.text if meta else ''

        if any(keyword in content.lower() for keyword in KEYWORDS) and is_recent(date_str):
            return {
                'url': url,
                'title': title,
                'published_date': date_str,
                'content': content[:1000]
            }
    except Exception as e:
        print(f"‚ùå Error extracting from {url}: {e}")
    return None

def scrape_category(category_name):
    print(f"\nüîç Scraping category: {category_name}")
    results = []
    urls = WEBSITE_URLS.get(category_name, [])
    for site_url in urls:
        if not isinstance(site_url, str) or not site_url.startswith("http"):
            continue
        print(f"üåê Site: {site_url}")
        article_links = extract_article_links(site_url)
        for link in article_links:
            article = extract_article_data(link)
            if article:
                results.append(article)
    return results
