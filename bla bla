# scraper.py (Optimized Threaded Version)

import requests
from bs4 import BeautifulSoup
from newspaper import Article
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from db_setup import ScrapedArticle, Session
from config import WEBSITE_URLS

MAX_ARTICLES_PER_SITE = 10
DAYS_LIMIT = 7
AI_KEYWORDS = [
    "ai", "gen ai", "generative ai", "gpt", "llm", "chatbot", "prompt", "prompt engineering",
    "fine-tuning", "tokens", "transformer", "autoregressive", "openai", "embedding",
    "instruction tuning", "langchain", "huggingface", "model alignment", "vector database",
    "agentic", "autogen", "multi-modal", "rlhf", "text-to-image", "text-to-speech", "codex",
    "palm", "mistral", "claude", "cohere", "anthropic", "context window", "rag", "semantic search"
]

now = datetime.utcnow()
one_week_ago = now - timedelta(days=DAYS_LIMIT)

def contains_ai_keywords(text, min_hits=2):
    if not text:
        return False
    text = text.lower()
    return sum(1 for keyword in AI_KEYWORDS if keyword in text) >= min_hits

def extract_links_from_page(site_url):
    try:
        resp = requests.get(site_url, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        anchors = soup.find_all('a', href=True)
        links = [a['href'] for a in anchors if a['href'].startswith('http') or a['href'].startswith('/')]
        full_links = [link if link.startswith('http') else site_url.rstrip('/') + '/' + link.lstrip('/') for link in links]
        return list(set(full_links))  # unique links
    except Exception as e:
        print(f"‚ùå Failed to extract links from {site_url}: {e}")
        return []

def process_article(link, category):
    session = Session()
    try:
        if session.query(ScrapedArticle).filter_by(url=link).first():
            return None

        article = Article(link)
        article.download()
        article.parse()

        if len(article.text.split()) < 300:
            return None

        if not contains_ai_keywords(article.title + ' ' + article.text):
            return None

        published = article.publish_date or now
        if published < one_week_ago:
            return None

        new_article = ScrapedArticle(
            category=category,
            title=article.title.strip(),
            url=link,
            summary=article.text[:1000],
            published_at=published
        )
        session.add(new_article)
        session.commit()
        print(f"‚úÖ {article.title}")
        return new_article
    except Exception as e:
        return None
    finally:
        session.close()

def fetch_articles(category):
    urls = WEBSITE_URLS.get(category, [])
    total_saved = 0

    for site_url in urls:
        print(f"\nüåê Scraping site: {site_url}")
        links = extract_links_from_page(site_url)

        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = []
            count = 0
            for link in links:
                if count >= MAX_ARTICLES_PER_SITE:
                    break
                future = executor.submit(process_article, link, category)
                futures.append(future)
                count += 1

            for future in futures:
                result = future.result()
                if result:
                    total_saved += 1

    print(f"\n‚úÖ {category}: {total_saved} Gen AI articles saved.")

if __name__ == "__main__":
    fetch_articles("CurrentTrends")
