from newspaper import Article, build
from datetime import datetime, timedelta
from db_setup import ScrapedArticle, Session
from config import RSS_URLS  # You can rename this to WEBSITE_URLS if you've updated config.py

MAX_ARTICLES = 15
now = datetime.now()
one_week_ago = now - timedelta(days=7)

AI_KEYWORDS = [
    "ai", "gen ai", "generative ai", "gpt", "llm", "chatbot", "prompt", "prompt engineering",
    "fine-tuning", "tokens", "transformer", "autoregressive", "openai", "embedding",
    "instruction tuning", "langchain", "huggingface", "model alignment", "vector database",
    "agentic", "autogen", "multi-modal", "rlhf", "text-to-image", "text-to-speech", "codex", 
    "palm", "mistral", "claude", "cohere", "anthropic", "context window", "rag", "semantic search"
]

def contains_ai_keywords(text, min_hits=2):
    if not text:
        return False
    text = text.lower()
    return sum(1 for keyword in AI_KEYWORDS if keyword in text) >= min_hits

def fetch_articles(category):
    session = Session()
    new_count = 0

    urls = RSS_URLS.get(category, [])
    if not urls:
        print(f"âŒ No URLs found for category: {category}")
        return

    for site_url in urls:
        try:
            paper = build(site_url, memoize_articles=False)
            print(f"ðŸ” Checking {len(paper.articles)} articles at {site_url}")
            count = 0

            for content in paper.articles:
                if count >= MAX_ARTICLES:
                    break

                try:
                    content.download()
                    content.parse()

                    published = content.publish_date or now
                    if published < one_week_ago:
                        print(f"â³ Skipped (old): {content.url}")
                        continue

                    if session.query(ScrapedArticle).filter_by(url=content.url).first():
                        print(f"âš ï¸ Already in DB: {content.url}")
                        continue

                    title = content.title or ""
                    text = content.text or ""

                    if not contains_ai_keywords(title + " " + text):
                        print(f"ðŸš« No AI relevance: {content.url}")
                        continue

                    article = ScrapedArticle(
                        category=category,
                        title=title.strip(),
                        url=content.url,
                        summary=text[:1000],
                        published_at=published
                    )

                    session.add(article)
                    count += 1
                    new_count += 1
                    print(f"âœ… Added: {content.url}")

                except Exception as e:
                    print(f"âŒ Error scraping: {content.url}\n{e}")

        except Exception as e:
            print(f"âŒ Error accessing site: {site_url}\n{e}")

    session.commit()
    session.close()
    print(f"\nâœ… {category}: {new_count} new Gen AIâ€“focused articles scraped.")
