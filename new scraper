# scraper.py
import newspaper
from newspaper import Article
from datetime import datetime, timedelta
from db_setup import ScrapedArticle, Session
from config import WEBSITE_URLS

AI_KEYWORDS = [
    "ai", "gen ai", "generative ai", "gpt", "llm", "chatbot", "prompt", "prompt engineering",
    "fine-tuning", "tokens", "transformer", "autoregressive", "openai", "embedding",
    "instruction tuning", "langchain", "huggingface", "model alignment", "vector database",
    "agentic", "autogen", "multi-modal", "rlhf", "text-to-image", "text-to-speech", "codex", 
    "palm", "mistral", "claude", "cohere", "anthropic", "context window", "rag", "semantic search"
]

now = datetime.utcnow()
one_week_ago = now - timedelta(days=7)

MAX_ARTICLES = 10


def contains_ai_keywords(text, min_hits=2):
    if not text:
        return False
    text = text.lower()
    return sum(1 for keyword in AI_KEYWORDS if keyword in text) >= min_hits


def fetch_articles(category):
    session = Session()
    urls = WEBSITE_URLS.get(category, [])
    new_count = 0

    if not urls:
        print(f"âŒ No website URLs found for category: {category}")
        return

    print(f"ðŸ”Ž Scraping category: {category} ({len(urls)} websites)")

    for site_url in urls:
        try:
            paper = newspaper.build(site_url, memoize_articles=False)
            count = 0
            print(f"ðŸŒ Site: {site_url} ({len(paper.articles)} articles)")

            for article in paper.articles:
                if count >= MAX_ARTICLES:
                    break
                try:
                    article.download()
                    article.parse()
                    article.nlp()
                except:
                    continue

                title = article.title or ""
                text = article.text or ""
                url = article.url or ""
                publish_date = article.publish_date or now

                if publish_date < one_week_ago:
                    continue

                if not contains_ai_keywords(title + " " + text):
                    continue

                if session.query(ScrapedArticle).filter_by(url=url).first():
                    continue

                new_article = ScrapedArticle(
                    category=category,
                    title=title.strip(),
                    url=url,
                    summary=text[:1000],
                    published_at=publish_date
                )
                session.add(new_article)
                count += 1
                new_count += 1
                print(f"âœ… Saved: {title}")

        except Exception as e:
            print(f"âŒ Failed to scrape {site_url}: {e}")

    session.commit()
    session.close()
    print(f"\nâœ… {category}: {new_count} Gen AI articles saved.\n")
