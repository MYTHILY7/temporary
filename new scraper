from newspaper import Article, build
from datetime import datetime, timedelta, timezone
from db_setup import ScrapedArticle, Session
from config import RSS_URLS

MAX_PER_SITE = 10
now = datetime.now(timezone.utc)
one_week_ago = now - timedelta(days=7)

AI_KEYWORDS = [
    "ai", "gen ai", "generative ai", "gpt", "llm", "chatbot", "prompt", "openai",
    "langchain", "huggingface", "transformer", "fine-tuning", "embedding", "codex", 
    "mistral", "claude", "anthropic", "cohere", "semantic search", "vector db", "rag"
]

def has_ai_keywords(text):
    text = text.lower()
    return any(k in text for k in AI_KEYWORDS)

def fetch_articles(category):
    session = Session()
    urls = RSS_URLS.get(category, [])
    added = 0

    for site_url in urls:
        try:
            paper = build(site_url, memoize_articles=False)
            print(f"\nüåç {category} - Site: {site_url} ({len(paper.articles)} articles found)")

            for article in paper.articles[:MAX_PER_SITE]:
                try:
                    article.download()
                    article.parse()

                    if article.meta_lang and article.meta_lang != "en":
                        print(f"üåê Skipped non-English: {article.url}")
                        continue

                    published_at = article.publish_date
                    if published_at and published_at.tzinfo is None:
                        published_at = published_at.replace(tzinfo=timezone.utc)

                    if published_at and published_at < one_week_ago:
                        continue

                    if session.query(ScrapedArticle).filter_by(url=article.url).first():
                        continue

                    title = article.title or ""
                    text = article.text or ""
                    if not has_ai_keywords(title + " " + text):
                        continue

                    session.add(ScrapedArticle(
                        category=category,
                        title=title.strip(),
                        url=article.url,
                        summary=text[:1000],
                        published_at=published_at or now
                    ))

                    added += 1
                    print(f"‚úÖ Added: {title[:60]}")

                except Exception as e:
                    print(f"‚ö†Ô∏è Failed article: {e}")

        except Exception as e:
            print(f"‚ùå Failed to build site {site_url}: {e}")

    session.commit()
    session.close()
    print(f"\n‚úÖ {category} done ‚Äî {added} new articles.")

if __name__ == "__main__":
    fetch_articles("CurrentTrends")
