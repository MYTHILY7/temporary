import feedparser
from newspaper import Article
from datetime import datetime, timedelta
from db_setup import ScrapedArticle, Session
from config import RSS_URLS

now = datetime.now()
one_week_ago = now - timedelta(days=7)
MAX_ARTICLES = 15

AI_KEYWORDS = [
    "ai", "gen ai", "generative ai", "gpt", "llm", "chatbot", "prompt", "prompt engineering",
    "fine-tuning", "tokens", "transformer", "autoregressive", "openai", "embedding",
    "instruction tuning", "langchain", "huggingface", "model alignment", "vector database",
    "agentic", "autogen", "multi-modal", "rlhf", "text-to-image", "text-to-speech", "codex", 
    "palm", "mistral", "claude", "cohere", "anthropic", "context window", "rag", "semantic search"
]

def contains_ai_keywords(text, min_hits=2):
    if not text:
        return False
    text = text.lower()
    return sum(1 for keyword in AI_KEYWORDS if keyword in text) >= min_hits

def fetch_articles_from_rss(category):
    session = Session()
    new_count = 0
    urls = RSS_URLS.get(category, [])

    if not urls:
        print(f"âŒ No RSS URLs found for category: {category}")
        return

    for rss_url in urls:
        try:
            feed = feedparser.parse(rss_url)
            print(f"\nğŸ“¡ Reading feed: {rss_url} ({len(feed.entries)} items)")
            count = 0

            for entry in feed.entries:
                if count >= MAX_ARTICLES:
                    break

                title = entry.title if 'title' in entry else ""
                summary = entry.summary if 'summary' in entry else ""
                link = entry.link if 'link' in entry else ""

                # Parse published date
                try:
                    published = datetime(*entry.published_parsed[:6])
                except:
                    published = now

                if published < one_week_ago:
                    continue

                if not contains_ai_keywords(title + " " + summary):
                    continue

                if session.query(ScrapedArticle).filter_by(url=link).first():
                    continue

                # Download full article content if passed filters
                try:
                    article = Article(link)
                    article.download()
                    article.parse()
                    text = article.text or ""
                    if len(text.split()) < 300:
                        continue
                except:
                    continue

                # Save to DB
                new_article = ScrapedArticle(
                    category=category,
                    title=article.title.strip(),
                    url=link,
                    summary=text[:1000],
                    published_at=published
                )
                session.add(new_article)
                count += 1
                new_count += 1
                print(f"âœ… {title}")

        except Exception as e:
            print(f"âŒ Failed RSS feed: {rss_url}\n{e}")

    session.commit()
    session.close()
    print(f"\nâœ… {category}: {new_count} Gen AI articles saved.")

# Example usage
if __name__ == "__main__":
    fetch_articles_from_rss("CurrentTrends")
