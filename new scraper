from newspaper import Article, build
from datetime import datetime, timedelta
from db_setup import ScrapedArticle, Session
from config import WEBSITE_URLS

AI_KEYWORDS = [
    "ai", "gen ai", "generative ai", "gpt", "llm", "chatbot", "prompt", "prompt engineering",
    "fine-tuning", "tokens", "transformer", "autoregressive", "openai", "embedding",
    "instruction tuning", "langchain", "huggingface", "model alignment", "vector database",
    "agentic", "autogen", "multi-modal", "rlhf", "text-to-image", "text-to-speech", "codex", 
    "palm", "mistral", "claude", "cohere", "anthropic", "context window", "rag", "semantic search"
]

MAX_ARTICLES = 10
now = datetime.now()
one_week_ago = now - timedelta(days=7)

def contains_ai_keywords(text, min_hits=2):
    if not text:
        return False
    text = text.lower()
    return sum(1 for keyword in AI_KEYWORDS if keyword in text) >= min_hits

def fetch_articles_from_website(url, category):
    print(f"\nðŸŒ Visiting: {url}")
    paper = build(url, memoize_articles=False)
    session = Session()
    saved = 0

    for article in paper.articles:
        if saved >= MAX_ARTICLES:
            break
        try:
            article.download()
            article.parse()

            if not contains_ai_keywords(article.title + " " + article.text):
                continue

            if len(article.text.split()) < 300:
                continue

            published = article.publish_date or now
            if published < one_week_ago:
                continue

            if session.query(ScrapedArticle).filter_by(url=article.url).first():
                continue

            new_article = ScrapedArticle(
                category=category,
                title=article.title.strip(),
                url=article.url,
                summary=article.text[:1000],
                published_at=published
            )
            session.add(new_article)
            saved += 1
            print(f"âœ… Saved: {article.title}")

        except Exception:
            continue

    session.commit()
    session.close()
    print(f"ðŸ” Done: {saved} articles saved from {url}")

def fetch_articles():
    for category, urls in WEBSITE_URLS.items():
        for url in urls:
            fetch_articles_from_website(url, category)

if __name__ == "__main__":
    fetch_articles()
